{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Football Game Result Prediction in the English Premier League\n",
    "\n",
    "## Jong-Won Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "- Datasets for the respective season from http://www.football-data.co.uk/englandm.php\n",
    "- Combined dataset contains all 380 games of the seasons from 2014/2015 until 2018/2019\n",
    "- The test set is the season 2019/2020 with just 288 games due to the virus outbreak\n",
    "- Training data: 1900 rows and 68 columns\n",
    "- Test data: 288 rows and 106 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "games = pd.read_csv('epl_season_2014-2019.csv')\n",
    "\n",
    "#remove betting attributes\n",
    "games.drop(games.iloc[:, 23:68], inplace = True, axis = 1)\n",
    "\n",
    "#remove league since all are division 0\n",
    "games.drop(columns='Div', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "games.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Date\n",
    "\n",
    "* HomeTeam = Home Team\n",
    "* AwayTeam = Away Team\n",
    "\n",
    "\n",
    "* FTHG = Full Time Home Team Goals\n",
    "* FTAG = Full Time Away Team Goals\n",
    "* **FTR = Full Time Result (H=Home Win, D=Draw, A=Away Win)**\n",
    "\n",
    "\n",
    "* HTHG = Half Time Home Team Goals\n",
    "* HTAG = Half Time Away Team Goals\n",
    "* HTR = Half Time Result (H=Home Win, D=Draw, A=Away Win)\n",
    "\n",
    "\n",
    "* Referee = Match Referee\n",
    "\n",
    "\n",
    "* HS = Home Team Shots\n",
    "* AS = Away Team Shots\n",
    "\n",
    "\n",
    "* HST = Home Team Shots on Target\n",
    "* AST = Away Team Shots on Target\n",
    "\n",
    "\n",
    "* HC = Home Team Corners\n",
    "* AC = Away Team Corners\n",
    "\n",
    "\n",
    "* HF = Home Team Fouls Committed\n",
    "* AF = Away Team Fouls Committed\n",
    "\n",
    "\n",
    "* HY = Home Team Yellow Cards\n",
    "* AY = Away Team Yellow Cards\n",
    "\n",
    "\n",
    "* HR = Home Team Red Cards\n",
    "* AR = Away Team Red Cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess for Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_data = games.copy()\n",
    "\n",
    "# extract target label and drop NaN row 380\n",
    "games = games.drop(games.index[380])\n",
    "games_data = games_data.drop(games_data.index[380])\n",
    "games_target = games_data['FTR']\n",
    "\n",
    "\n",
    "#transform FTR from target array into numeric values/labels same as in test set\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "games_target = label_encoder.fit_transform(games_target)\n",
    "print(games_target)\n",
    "\n",
    "#transform HTR, labeling for analysis, later with one hot for classification\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "games_data['HTR'] = label_encoder.fit_transform(games_data['HTR'])\n",
    "print(games_data['HTR'])\n",
    "\n",
    "#transform FTR, labeling for analysis before dropping\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "games_data['FTR'] = label_encoder.fit_transform(games_data['FTR'])\n",
    "print(games_data['FTR'])\n",
    "\n",
    "#transform Referee, labeling for analysis, later droppedd\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "games_data['Referee'] = label_encoder.fit_transform(games_data['Referee'])\n",
    "print(games_data['Referee'])\n",
    "\n",
    "#transform Date, ordinal labeling for analysis,\n",
    "ordinal_encoder = preprocessing.OrdinalEncoder()\n",
    "games_data['Date'] = ordinal_encoder.fit_transform(games_data['Date'].values.reshape(-1,1))\n",
    "print(games_data['Date'])\n",
    "\n",
    "games_data.head(20)\n",
    "#games_target[380]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting shot attributes\n",
    "\n",
    "pd.plotting.scatter_matrix(games_data[['HS', 'HF']], figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting shot attributes\n",
    "plt.scatter(games_data['HC'], games_data['HS'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting shots and corners\n",
    "pd.plotting.scatter_matrix(games[['HS','AS','HST','AST','HC','AC']], figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Statistics and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = away team win\n",
    "# 1 = draw\n",
    "# 2 = home team win\n",
    "\n",
    "# looking at the distribution of the target variable FTR\n",
    "plt.hist(games_target)\n",
    "\n",
    "matches = games.shape[0]\n",
    "\n",
    "homewins = len(games[games.FTR == 'H'])\n",
    "winrate_home = (homewins / matches) * 100\n",
    "\n",
    "awaywins = len(games[games.FTR == 'A'])\n",
    "winrate_away = (awaywins / matches) * 100\n",
    "\n",
    "draws = len(games[games.FTR == 'D'])\n",
    "winrate_draw = (draws / matches) * 100\n",
    "\n",
    "print(\"Number of matches: {}\".format(matches))\n",
    "print()\n",
    "print(\"Number of homewins: {}\".format(homewins))\n",
    "print(\"Winrate of hometeams: {:.2f}%\".format(winrate_home))\n",
    "print()\n",
    "print(\"Number of awaywins: {}\".format(awaywins))\n",
    "print(\"Winrate of awayteams: {:.2f}%\".format(winrate_away))\n",
    "print()\n",
    "print(\"Number of draws: {}\".format(draws))\n",
    "print(\"Winrate of draws: {:.2f}%\".format(winrate_draw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevance of half time result (HTR) for full time result (FTR)\n",
    "sum_homewin = 0;\n",
    "sum_awaywin = 0;\n",
    "sum_draw = 0;\n",
    "sum_change = 0;\n",
    "\n",
    "for i in range(1900):\n",
    "    if(i != 380 and games_target[i] == games_data.loc[i]['HTR']):\n",
    "        if(games_target[i] == 2):\n",
    "            sum_homewin += 1\n",
    "        if(games_target[i] == 0):\n",
    "            sum_awaywin += 1\n",
    "        if(games_target[i] == 1):\n",
    "            sum_draw += 1\n",
    "    else: sum_change += 1\n",
    "        \n",
    "print(\"Half time homewin leads to full time homewin: #{} = {:.2f}%\".format(sum_homewin, 100*(sum_homewin/homewins)))\n",
    "print(\"Half time awaywin leads to full time awaywin: #{} = {:.2f}%\".format(sum_awaywin, 100*(sum_awaywin/awaywins)))\n",
    "print(\"Half time draw leads to full time draw: #{} = {:.2f}%\".format(sum_draw, 100*(sum_draw/draws)))\n",
    "print(\"HTR != FTR: #{}\".format(sum_change))\n",
    "\n",
    "print(\"check sum of matches (1900): #{}\".format(sum_change+sum_homewin+sum_awaywin+sum_draw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_pp = games_data.copy()\n",
    "\n",
    "games_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values in training data\n",
    "print(games_pp.isnull().values.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at home team attribute\n",
    "print(games_pp['HomeTeam'].value_counts())\n",
    "print(games_pp['HomeTeam'].value_counts().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform hometeam and awayteam categorial values into numeric values by manually mapping with dictionary\n",
    "# to achieve also same labeling in test set\n",
    "# higher numbers might get higher weights: teams that played more seasons will get higher number\n",
    "List_A = ['Sheffield United','QPR', 'Fulham', 'Norwich', 'Cardiff', 'Wolves', 'Middlesbrough', 'Huddersfield', 'Hull', 'Brighton', 'Aston Villa', 'Sunderland', 'Bournemouth', 'Watford', 'Newcastle', 'West Brom', 'Swansea', 'Stoke', 'Burnley', 'Tottenham', 'Leicester', 'Southampton', 'Man City', 'Crystal Palace', 'Everton', 'Man United', 'Liverpool','West Ham', 'Arsenal', 'Chelsea']\n",
    "List_B = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29]\n",
    "d=dict(zip(List_A, List_B))\n",
    "print(d)\n",
    "\n",
    "# transform teams not with one hot to avoid curse of dimensionality? but same as in test set\n",
    "# higher values for more seasons, sheffield team for test set\n",
    "\n",
    "games_pp['HomeTeam'] = games_pp['HomeTeam'].map(d).fillna(games_pp['HomeTeam'])\n",
    "\n",
    "games_pp['AwayTeam'] = games_pp['AwayTeam'].map(d).fillna(games_pp['AwayTeam'])\n",
    "\n",
    "games_pp.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U pandas-profiling[notebook,html]\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(games_pp, title='Pandas Profiling Report', html={'style':{'full_width':True}})\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove full-time related attributes: yellow/red cards, fouls, shots, corners\n",
    "games_pp = games_pp.drop(columns=['HY','AY', 'HR', 'AR'])\n",
    "\n",
    "games_pp = games_pp.drop(columns=['HF','AF'])\n",
    "\n",
    "games_pp = games_pp.drop(columns=['HS','AS', 'HST', 'AST', 'HC', 'AC'])\n",
    "\n",
    "# remove referee and date since no correlation\n",
    "games_pp = games_pp.drop(columns=['Referee'])\n",
    "games_pp.drop(columns='Date', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# remove full time goals\n",
    "games_pp = games_pp.drop(columns=['FTHG', 'FTAG'])\n",
    "\n",
    "# drop target\n",
    "games_pp = games_pp.drop(columns='FTR')\n",
    "\n",
    "#encode half time result HTR with one hot\n",
    "encoder = preprocessing.OneHotEncoder()\n",
    "games_onehot = pd.DataFrame(encoder.fit_transform(games['HTR'].values.reshape(-1,1)).toarray(), columns=encoder.get_feature_names(['HTR']))\n",
    "\n",
    "print(games_onehot)\n",
    "\n",
    "games_pp = games_pp.drop(columns=['HTR'])\n",
    "games_pp = games_pp.join(games_onehot)\n",
    "games_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale all numeric values to same range with MinMaxScaler\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "games_pp[['HTHG', 'HTAG']] = scaler.fit_transform(games_pp[['HTHG', 'HTAG']])\n",
    "\n",
    "games_pp\n",
    "\n",
    "#remove NaN (there is one in line 1900)\n",
    "games_pp = games_pp.drop(games_pp.index[1899])\n",
    "print(games_pp.isnull().values.sum())\n",
    "print(np.isnan(games_target).any())\n",
    "games_target = np.delete(games_target, 1899)\n",
    "\n",
    "games_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_test = pd.read_csv('test_season_2019-2020.csv')\n",
    "#remove betting attributes\n",
    "games_test.drop(games_test.iloc[:, 24:106], inplace = True, axis = 1)\n",
    "\n",
    "#remove league and date and time\n",
    "games_test.drop(columns='Div', axis=1, inplace=True)\n",
    "games_test.drop(columns='Date', axis=1, inplace=True)\n",
    "games_test.drop(columns='Time', axis=1, inplace=True)\n",
    "\n",
    "#drop target\n",
    "games_target_test = games_test['FTR']\n",
    "games_test = games_test.drop(columns='FTR')\n",
    "\n",
    "#transform FTR #2=H,1=D,0=A but same as in test set\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "games_target_test = label_encoder.fit_transform(games_target_test)\n",
    "print(games_target_test)\n",
    "\n",
    "# removed yellow and red cards\n",
    "# removed fouls and referee\n",
    "# removed shots and corners\n",
    "games_test = games_test.drop(columns=['HF','AF'])\n",
    "games_test = games_test.drop(columns=['HY','AY', 'HR', 'AR'])\n",
    "games_test= games_test.drop(columns=['Referee'])\n",
    "games_test = games_test.drop(columns=['HS','AS', 'HST', 'AST', 'HC', 'AC'])\n",
    "\n",
    "#remove full time goals\n",
    "games_test = games_test.drop(columns=['FTHG', 'FTAG'])\n",
    "\n",
    "#onehot HTR\n",
    "games_onehot = pd.DataFrame(encoder.fit_transform(games_test['HTR'].values.reshape(-1,1)).toarray(), columns=encoder.get_feature_names(['HTR']))\n",
    "games_test = games_test.drop(columns=['HTR'])\n",
    "games_test = games_test.join(games_onehot)\n",
    "\n",
    "#normalising\n",
    "games_test[['HTHG', 'HTAG']] = scaler.fit_transform(games_test[['HTHG', 'HTAG']])\n",
    "\n",
    "# transforming home and awayteam with mapping from training set\n",
    "# higher values for more seasons, sheffield team for test set\n",
    "# transform teams not with one hot to avoid curse of dimensionality\n",
    "games_test['HomeTeam'] = games_test['HomeTeam'].map(d).fillna(games_test['HomeTeam'])\n",
    "\n",
    "games_test['AwayTeam'] = games_test['AwayTeam'].map(d).fillna(games_test['AwayTeam'])\n",
    "\n",
    "games_test.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Statistics / Profiling Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test data set just for analysis\n",
    "games_test_c = pd.read_csv('test_season_2019-2020.csv')\n",
    "games_test_c.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# analyze and check team values\n",
    "print(games_test['HomeTeam'].value_counts())\n",
    "print(games_test['HomeTeam'].value_counts().count())\n",
    "\n",
    "\n",
    "#compute panda profilling report\n",
    "%pip install -q -U pandas-profiling[notebook,html]\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(games_test, title='Pandas Profiling Report', html={'style':{'full_width':True}})\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = away team win\n",
    "# 1 = draw\n",
    "# 2 = home team win\n",
    "\n",
    "# looking at distribution of the class label\n",
    "plt.hist(games_target_test)\n",
    "\n",
    "matches = games_test_c.shape[0]\n",
    "\n",
    "homewins = len(games_test_c[games_test_c.FTR == 'H'])\n",
    "winrate_home = (homewins / matches) * 100\n",
    "\n",
    "awaywins = len(games_test_c[games_test_c.FTR == 'A'])\n",
    "winrate_away = (awaywins / matches) * 100\n",
    "\n",
    "draws = len(games_test_c[games_test_c.FTR == 'D'])\n",
    "winrate_draw = (draws / matches) * 100\n",
    "\n",
    "print(\"Number of matches: {}\".format(matches))\n",
    "print()\n",
    "print(\"Number of homewins: {}\".format(homewins))\n",
    "print(\"Winrate of hometeams: {:.2f}%\".format(winrate_home))\n",
    "print()\n",
    "print(\"Number of awaywins: {}\".format(awaywins))\n",
    "print(\"Winrate of awayteams: {:.2f}%\".format(winrate_away))\n",
    "print()\n",
    "print(\"Number of draws: {}\".format(draws))\n",
    "print(\"Winrate of draws: {:.2f}%\".format(winrate_draw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for NaN values\n",
    "games_test.head(10)\n",
    "print(games_pp.isnull().values.sum())\n",
    "check_for_nan2 = games_pp['HomeTeam'].isnull().sum()\n",
    "check_for_nan3 = games_pp['AwayTeam'].isnull().sum()\n",
    "nan_rows = games[games['HomeTeam'].isnull()]\n",
    "#print(nan_rows)\n",
    "nan_rows = games[games['AwayTeam'].isnull()]\n",
    "#print(nan_rows)\n",
    "\n",
    "games_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "* Decision Tree\n",
    "* KNN\n",
    "* GaussianNB, MultinomialNB\n",
    "* SVC\n",
    "* RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Classification with\n",
    "\n",
    "\n",
    "### Decision Tree, KNN, GaussianNB, SVC, MultinomialNB and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#baseline configuration, classification and evaluation\n",
    "pipeline = Pipeline([('estimator', DecisionTreeClassifier())])\n",
    "\n",
    "estimators = [\n",
    "    DecisionTreeClassifier(), KNeighborsClassifier(), GaussianNB(), SVC(gamma='auto'), MultinomialNB(), RandomForestClassifier()\n",
    "]\n",
    "\n",
    "# stratified 10 fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# evaluate accuracy and print out classification report and confusion matrix for each estimator\n",
    "for i, estimator in enumerate(estimators):\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.subplot(3,3,i+1)\n",
    "    pipeline.set_params(estimator=estimator)\n",
    "    prediction = cross_val_predict(pipeline, games_pp, games_target, cv=cv, n_jobs=-1)\n",
    "    cm = confusion_matrix(games_target, prediction)\n",
    "    plot_confusion_matrix(cm, classes=unique_labels(games_target), title=type(estimator).__name__)\n",
    "    accuracy= cross_val_score(pipeline, games_pp, games_target, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    print(type(estimator).__name__)\n",
    "    print(\"Accuracy = {}%\".format(accuracy.mean() * 100.0))\n",
    "    print(classification_report(games_target, prediction))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(games_pp, games_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%pip install -q -U graphviz\n",
    "\n",
    "#visualize tree\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] += ';C:\\\\Users\\\\joey_\\\\Desktop\\\\Jupyter Notebook\\\\graphviz-2.38\\\\release\\\\bin'\n",
    "\n",
    "import graphviz\n",
    "from sklearn import tree\n",
    "\n",
    "dot_data = tree.export_graphviz(dt, out_file=None, \n",
    "                                feature_names=games_pp.columns, \n",
    "                                class_names=label_encoder.classes_,\n",
    "                               filled=True, rounded=True, special_characters=True) \n",
    "\n",
    "graph = graphviz.Source(dot_data) \n",
    "\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validate with stratifiedkfold for mean accuracy\n",
    "cross_val = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "accuracy_dt= cross_val_score(dt, games_pp, games_target, cv=cross_val, scoring='accuracy')\n",
    "accuracy_dt.mean()\n",
    "\n",
    "print(accuracy_dt.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-val prediction for error analysis\n",
    "predicted = cross_val_predict(dt, games_pp, games_target, cv=10)\n",
    "\n",
    "#classification report with cross-validated prediction\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(games_target, predicted, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "parameters = {\n",
    "    'criterion':['gini', 'entropy'], \n",
    "    'max_depth':[ 2, 3, 4, 5, None],\n",
    "    'min_samples_split' :[2,3,4,5]\n",
    "}\n",
    "\n",
    "# specify the cross validation\n",
    "stratified_10_fold_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# create the grid search instance\n",
    "grid_search_estimator = GridSearchCV(dt, parameters, scoring='accuracy', cv=stratified_10_fold_cv, return_train_score=False)\n",
    "\n",
    "# run the grid search\n",
    "grid_search_estimator.fit(games_pp, games_target)\n",
    "\n",
    "# print the results of all hyper-parameter combinations\n",
    "results = pd.DataFrame(grid_search_estimator.cv_results_)\n",
    "display(results)\n",
    "    \n",
    "# print the best parameter setting\n",
    "print(\"best score is {} with params {}\".format(grid_search_estimator.best_score_, grid_search_estimator.best_params_))\n",
    "\n",
    "# cross-validate\n",
    "prediction = cross_val_predict(grid_search_estimator, games_pp, games_target, cv=cross_val, n_jobs=-1)\n",
    "\n",
    "# calculate\n",
    "cm = confusion_matrix(games_target, prediction)\n",
    "acc = accuracy_score(games_target, prediction)\n",
    "\n",
    "# print classification report and confusion matrix\n",
    "print(\"Optimised Decision Tree with accuracy of {}\".format(acc))\n",
    "plot_confusion_matrix(cm, classes=label_encoder.classes_, title='Decision Tree Classifier')\n",
    "plt.show()\n",
    "print(classification_report(games_target, prediction, target_names=label_encoder.classes_))\n",
    "\n",
    "# fit the grid search (= determine the optimal parameters)\n",
    "grid_search_estimator.fit(games_pp, games_target)\n",
    "print(\"Optimised Parameters: {}\".format(grid_search_estimator.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction with test set and tuned parameters\n",
    "dt = DecisionTreeClassifier(criterion= 'gini', max_depth= 5, min_samples_split= 3)\n",
    "dt.fit(games_pp, games_target)\n",
    "dt_prediction = dt.predict(games_test)\n",
    "\n",
    "cnf_matrix = confusion_matrix(games_target_test, dt_prediction)\n",
    "np.set_printoptions(precision=2)\n",
    "plot_confusion_matrix(cnf_matrix, classes=label_encoder.classes_)\n",
    "\n",
    "#evaluation with confusion matrix and accuracy\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(games_target_test, dt_prediction))\n",
    "print()\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy_score(games_target_test, dt_prediction))) #before: 52,08%\n",
    "\n",
    "%pip install -q -U graphviz\n",
    "\n",
    "#visualize new tree\n",
    "\n",
    "import os\n",
    "os.environ['PATH'] += ';C:\\\\Users\\\\joey_\\\\Desktop\\\\Jupyter Notebook\\\\graphviz-2.38\\\\release\\\\bin'\n",
    "\n",
    "import graphviz\n",
    "from sklearn import tree\n",
    "\n",
    "dot_data = tree.export_graphviz(dt, out_file=None, \n",
    "                                feature_names=games_pp.columns, \n",
    "                                class_names=label_encoder.classes_,\n",
    "                               filled=True, rounded=True, special_characters=True) \n",
    "\n",
    "graph = graphviz.Source(dot_data) \n",
    "\n",
    "display(graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(games_target_test, dt_prediction, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest-Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "knn = KNeighborsClassifier()\n",
    "accuracy_knn= cross_val_score(knn, games_pp, games_target, cv=cross_val, scoring='accuracy')\n",
    "accuracy_knn.mean()\n",
    "print(accuracy_knn.mean())\n",
    "print(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-val prediction\n",
    "predicted = cross_val_predict(knn, games_pp, games_target, cv=10)\n",
    "\n",
    "# cross-val prediction and classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(games_target, predicted, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-NN Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create an estimator\n",
    "knn_estimator = KNeighborsClassifier()\n",
    "\n",
    "# specify the parameter grid\n",
    "parameters = {\n",
    "    'n_neighbors': range(2, 9)\n",
    "}\n",
    "\n",
    "# specify the cross validation\n",
    "stratified_10_fold_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# create the grid search instance\n",
    "grid_search_estimator = GridSearchCV(knn_estimator, parameters, scoring='accuracy', cv=stratified_10_fold_cv, return_train_score=False)\n",
    "\n",
    "# run the grid search\n",
    "grid_search_estimator.fit(games_pp, games_target)\n",
    "\n",
    "# print the results of all hyper-parameter combinations\n",
    "results = pd.DataFrame(grid_search_estimator.cv_results_)\n",
    "display(results)\n",
    "    \n",
    "# print the best parameter setting\n",
    "print(\"best score is {} with params {}\".format(grid_search_estimator.best_score_, grid_search_estimator.best_params_))\n",
    "\n",
    "# cross-validate\n",
    "prediction = cross_val_predict(grid_search_estimator, games_pp, games_target, cv=cross_val, n_jobs=-1)\n",
    "\n",
    "# calculate\n",
    "cm = confusion_matrix(games_target, prediction)\n",
    "acc = accuracy_score(games_target, prediction)\n",
    "\n",
    "# print classification report and confusion matrix\n",
    "print(\"Optimised KNN with accuracy of {}\".format(acc))\n",
    "plot_confusion_matrix(cm, classes=label_encoder.classes_, title='KNN')\n",
    "plt.show()\n",
    "print(classification_report(games_target, prediction, target_names=label_encoder.classes_))\n",
    "\n",
    "# fit the grid search (= determine the optimal parameters)\n",
    "grid_search_estimator.fit(games_pp, games_target)\n",
    "print(\"Optimised Parameters: {}\".format(grid_search_estimator.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction with test data and tuned parameters\n",
    "knn = KNeighborsClassifier(7)\n",
    "knn.fit(games_pp, games_target)\n",
    "knn_prediction = knn.predict(games_test)\n",
    "\n",
    "cnf_matrix = confusion_matrix(games_target_test, knn_prediction)\n",
    "np.set_printoptions(precision=2)\n",
    "plot_confusion_matrix(cnf_matrix, classes=label_encoder.classes_)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(games_target_test, knn_prediction))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy_score(games_target_test, knn_prediction))) #before: 51,04%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(games_target_test, knn_prediction, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create estimator\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(games_pp, games_target)\n",
    "print(gnb)\n",
    "print('----------------CV-------------------')\n",
    "\n",
    "# evaluation with training data\n",
    "cross_val = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "accuracy_gnb= cross_val_score(gnb, games_pp, games_target, cv=cross_val, scoring='accuracy')\n",
    "accuracy_gnb.mean()\n",
    "\n",
    "print('Accuracy:')\n",
    "print(accuracy_gnb.mean())\n",
    "\n",
    "predicted = cross_val_predict(gnb, games_pp, games_target, cv=10)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(games_target, predicted, target_names=label_encoder.classes_))\n",
    "\n",
    "print('--------------TEST---------------------')\n",
    "#with test data and no hyperparameters\n",
    "gnb_prediction = gnb.predict(games_test)\n",
    "\n",
    "cnf_matrix = confusion_matrix(games_target_test, gnb_prediction)\n",
    "np.set_printoptions(precision=2)\n",
    "plot_confusion_matrix(cnf_matrix, classes=label_encoder.classes_)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(games_target_test, gnb_prediction))\n",
    "print()\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy_score(games_target_test, gnb_prediction)))\n",
    "#before optimisation 57,99%\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(games_target_test, gnb_prediction, target_names=label_encoder.classes_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create estimator\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(games_pp, games_target)\n",
    "print(mnb)\n",
    "print('----------------CV-------------------')\n",
    "\n",
    "# evaluation with training data\n",
    "cross_val = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "accuracy_mnb= cross_val_score(mnb, games_pp, games_target, cv=cross_val, scoring='accuracy')\n",
    "accuracy_mnb.mean()\n",
    "\n",
    "print('Accuracy:')\n",
    "print(accuracy_mnb.mean())\n",
    "\n",
    "predicted = cross_val_predict(mnb, games_pp, games_target, cv=10)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(games_target, predicted, target_names=label_encoder.classes_))\n",
    "\n",
    "print('---------------TEST--------------------')\n",
    "#with test and tuned parameters\n",
    "mnb = MultinomialNB(alpha=0)\n",
    "mnb.fit(games_pp, games_target)\n",
    "mnb_prediction = mnb.predict(games_test)\n",
    "\n",
    "cnf_matrix = confusion_matrix(games_target_test, mnb_prediction)\n",
    "np.set_printoptions(precision=2)\n",
    "plot_confusion_matrix(cnf_matrix, classes=label_encoder.classes_)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(games_target_test, mnb_prediction))\n",
    "print()\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy_score(games_target_test, mnb_prediction)))\n",
    "#before optimisiation 50%\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(games_target_test, mnb_prediction, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomialNB Hyperparameter Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MultinomialNB without laplace smoothing\n",
    "\n",
    "mnb1 = MultinomialNB(alpha=0)\n",
    "mnb1.fit(games_pp, games_target)\n",
    "print(mnb1)\n",
    "print('---------------CV--------------------')\n",
    "\n",
    "#with training\n",
    "cross_val = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "accuracy_mnb1= cross_val_score(mnb1, games_pp, games_target, cv=cross_val, scoring='accuracy')\n",
    "accuracy_mnb1.mean()\n",
    "\n",
    "print('Accuracy:')\n",
    "print(accuracy_mnb1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create estimator\n",
    "svc = SVC(gamma='auto')\n",
    "svc.fit(games_pp, games_target)\n",
    "print(svc)\n",
    "print('---------------CV------------------')\n",
    "\n",
    "# evaluation with training data\n",
    "cross_val = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "accuracy_svc= cross_val_score(svc, games_pp, games_target, cv=cross_val, scoring='accuracy')\n",
    "accuracy_svc.mean()\n",
    "\n",
    "print('Accuracy:')\n",
    "print(accuracy_svc.mean())\n",
    "\n",
    "predicted = cross_val_predict(svc, games_pp, games_target, cv=10)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(games_target, predicted, target_names=label_encoder.classes_))\n",
    "\n",
    "print('--------------TEST (tuned)---------------------')\n",
    "#with test and tuned parameters\n",
    "\n",
    "svc = SVC(C=1, gamma=0.1, kernel='linear')\n",
    "svc.fit(games_pp, games_target)\n",
    "svc_prediction = svc.predict(games_test)\n",
    "\n",
    "cnf_matrix = confusion_matrix(games_target_test, svc_prediction)\n",
    "np.set_printoptions(precision=2)\n",
    "plot_confusion_matrix(cnf_matrix, classes=label_encoder.classes_)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(games_target_test, svc_prediction))\n",
    "print()\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy_score(games_target_test, svc_prediction)))\n",
    "#before optimisiation 50,69%\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(games_target_test, svc_prediction, target_names=label_encoder.classes_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Hyperparamter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { #cant handle too many parameters, too much computing power needed\n",
    "    'kernel':['linear'], #rbf', 'poly'\n",
    "    'gamma':[0.1, 1], #10, 100\n",
    "    'C' :[0.1, 1] #10, 100, 1000\n",
    "}\n",
    "\n",
    "# specify the cross validation\n",
    "stratified_10_fold_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# create the grid search instance\n",
    "grid_search_estimator = GridSearchCV(svc, parameters, scoring='accuracy', cv=stratified_10_fold_cv, return_train_score=False)\n",
    "\n",
    "# run the grid search\n",
    "grid_search_estimator.fit(games_pp, games_target)\n",
    "\n",
    "# print the results of all hyper-parameter combinations\n",
    "results = pd.DataFrame(grid_search_estimator.cv_results_)\n",
    "display(results)\n",
    "    \n",
    "# print the best parameter setting\n",
    "print(\"best score is {} with params {}\".format(grid_search_estimator.best_score_, grid_search_estimator.best_params_))\n",
    "\n",
    "# cross-validate\n",
    "prediction = cross_val_predict(grid_search_estimator, games_pp, games_target, cv=cv, n_jobs=-1)\n",
    "\n",
    "# calculate\n",
    "cm = confusion_matrix(games_target, prediction)\n",
    "acc = accuracy_score(games_target, prediction)\n",
    "\n",
    "# print classification matrix and confusion matrix\n",
    "print(\"Optimised SVC with accuracy of {}\".format(acc))\n",
    "plot_confusion_matrix(cm, classes=label_encoder.classes_, title='SVC')\n",
    "plt.show()\n",
    "print(classification_report(games_target, prediction, target_names=label_encoder.classes_))\n",
    "\n",
    "# fit the grid search (= determine the optimal parameters)\n",
    "grid_search_estimator.fit(games_pp, games_target)\n",
    "print(\"Optimised Parameters: {}\".format(grid_search_estimator.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#create estimator\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(games_pp, games_target)\n",
    "#print(rf)\n",
    "print('---------------CV-------------------------')\n",
    "\n",
    "# evaluation with training data\n",
    "cross_val = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "accuracy_rf= cross_val_score(rf, games_pp, games_target, cv=cross_val, scoring='accuracy')\n",
    "accuracy_rf.mean()\n",
    "\n",
    "print('Accuracy:')\n",
    "print(accuracy_rf.mean())\n",
    "\n",
    "predicted = cross_val_predict(rf, games_pp, games_target, cv=10)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(games_target, predicted, target_names=label_encoder.classes_))\n",
    "\n",
    "print('-----------------TEST------------------')\n",
    "#with test and tuned parameters\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap = True, max_depth = 100, max_features = 'sqrt', min_samples_leaf = 4, min_samples_split=5,n_estimators=100)\n",
    "rf.fit(games_pp, games_target)\n",
    "rf_prediction = rf.predict(games_test)\n",
    "print(rf)\n",
    "\n",
    "cnf_matrix = confusion_matrix(games_target_test, rf_prediction)\n",
    "np.set_printoptions(precision=2)\n",
    "plot_confusion_matrix(cnf_matrix, classes=label_encoder.classes_)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(games_target_test, rf_prediction))\n",
    "print()\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy_score(games_target_test, rf_prediction)))\n",
    "accuracy_rf= cross_val_score(rf, games_test, games_target_test, cv=cross_val, scoring='accuracy')\n",
    "accuracy_rf.mean()\n",
    "\n",
    "print('Mean Accuracy:')\n",
    "print(accuracy_rf.mean())\n",
    "\n",
    "#before 57,64%\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(games_target_test, rf_prediction, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Hyperparamter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { #cant handle too many parameters, too much computing power needed\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [50, 100, None],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [1,4],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'n_estimators': [100, 200]\n",
    "}\n",
    "\n",
    "{}\n",
    "\n",
    "# specify the cross validation\n",
    "stratified_10_fold_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# create the grid search instance\n",
    "grid_search_estimator = GridSearchCV(rf, parameters, scoring='accuracy', cv=stratified_10_fold_cv, return_train_score=False)\n",
    "\n",
    "# run the grid search\n",
    "grid_search_estimator.fit(games_pp, games_target)\n",
    "\n",
    "# print the results of all hyper-parameter combinations\n",
    "results = pd.DataFrame(grid_search_estimator.cv_results_)\n",
    "display(results)\n",
    "    \n",
    "# print the best parameter setting\n",
    "print(\"best score is {} with params {}\".format(grid_search_estimator.best_score_, grid_search_estimator.best_params_))\n",
    "\n",
    "# cross-validate\n",
    "prediction = cross_val_predict(grid_search_estimator, games_pp, games_target, cv=cv, n_jobs=-1)\n",
    "\n",
    "# calculate\n",
    "cm = confusion_matrix(games_target, prediction)\n",
    "acc = accuracy_score(games_target, prediction)\n",
    "\n",
    "# print classification report and confusion matrix\n",
    "print(\"Optimised Random Forest with accuracy of {}\".format(acc))\n",
    "plot_confusion_matrix(cm, classes=label_encoder.classes_, title='RF')\n",
    "plt.show()\n",
    "print(classification_report(games_target, prediction, target_names=label_encoder.classes_))\n",
    "\n",
    "# fit the grid search (= determine the optimal parameters)\n",
    "grid_search_estimator.fit(games_pp, games_target)\n",
    "print(\"Optimised Parameters: {}\".format(grid_search_estimator.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* random guessing = 33.33%\n",
    "\n",
    "* assigning every class to homewin = 44.79%\n",
    "\n",
    "* all classifiers are better at predicting homewin and awaywin than predicting major class but more difficult to predict draws\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set with New Attribute Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new dataset with new ranking attribute\n",
    "games_ranking = pd.read_csv('epl_season_2014-2019_ranking.csv')\n",
    "\n",
    "# remove betting attributes and rename new columns\n",
    "games_ranking.drop(games_ranking.iloc[:, 23:68], inplace = True, axis = 1)\n",
    "games_ranking.rename(columns={'HomeTeamPerviousRanking': 'HomePreviousRanking', 'HomeAwayPerviousRanking':'AwayPreviousRanking'}, inplace=True)\n",
    "games_ranking.head()\n",
    "\n",
    "# remove league and date\n",
    "games_ranking.drop(columns='Div', axis=1, inplace=True)\n",
    "games_ranking.drop(columns='Date', axis=1, inplace=True)\n",
    "\n",
    "# drop target\n",
    "games_target_ranking = games_ranking['FTR']\n",
    "games_ranking = games_ranking.drop(columns='FTR')\n",
    "\n",
    "# transform FTR #2=H,1=D,0=A but same as in test set\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "games_target_ranking = label_encoder.fit_transform(games_target_ranking)\n",
    "\n",
    "# removed yellow and red cards\n",
    "# removed fouls and referee\n",
    "# removed shots and corners\n",
    "games_ranking = games_ranking.drop(columns=['HF','AF'])\n",
    "games_ranking = games_ranking.drop(columns=['HY','AY', 'HR', 'AR'])\n",
    "games_ranking = games_ranking.drop(columns=['Referee'])\n",
    "games_ranking = games_ranking.drop(columns=['HS','AS', 'HST', 'AST', 'HC', 'AC'])\n",
    "\n",
    "# remove full time goals\n",
    "games_ranking = games_ranking.drop(columns=['FTHG', 'FTAG'])\n",
    "\n",
    "# onehot HTR\n",
    "games_onehot_ranking = pd.DataFrame(encoder.fit_transform(games_ranking['HTR'].values.reshape(-1,1)).toarray(), columns=encoder.get_feature_names(['HTR']))\n",
    "games_ranking = games_ranking.drop(columns=['HTR'])\n",
    "games_ranking = games_ranking.join(games_onehot_ranking)\n",
    "\n",
    "# normalising + new attribute\n",
    "games_ranking[['HTHG', 'HTAG','HomePreviousRanking', 'AwayPreviousRanking']] = scaler.fit_transform(games_ranking[['HTHG', 'HTAG','HomePreviousRanking', 'AwayPreviousRanking']])\n",
    "\n",
    "# transforming home and awayteam with mapping from training set\n",
    "# higher values for more seasons, sheffield team for test set\n",
    "# transform teams not with one hot to avoid curse of dimensionality\n",
    "games_ranking['HomeTeam'] = games_ranking['HomeTeam'].map(d).fillna(games_ranking['HomeTeam'])\n",
    "\n",
    "games_ranking['AwayTeam'] = games_ranking['AwayTeam'].map(d).fillna(games_ranking['AwayTeam'])\n",
    "\n",
    "games_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new test with new ranking attribute\n",
    "games_test_ranking = pd.read_csv('test_season_2019-2020_ranking.csv')\n",
    "games_test_ranking.rename(columns={'HomeTeamRankings': 'HomePreviousRanking', 'AwayTeamRankings':'AwayPreviousRanking'}, inplace=True)\n",
    "\n",
    "# remove betting attributes\n",
    "games_test_ranking.drop(games_test_ranking.iloc[:, 24:106], inplace = True, axis = 1)\n",
    "\n",
    "# remove league and date and time\n",
    "games_test_ranking.drop(columns='Div', axis=1, inplace=True)\n",
    "games_test_ranking.drop(columns='Date', axis=1, inplace=True)\n",
    "games_test_ranking.drop(columns='Time', axis=1, inplace=True)\n",
    "\n",
    "# drop target\n",
    "games_target_test_ranking = games_test_ranking['FTR']\n",
    "games_test_ranking = games_test_ranking.drop(columns='FTR')\n",
    "\n",
    "# transform FTR #2=H,1=D,0=A but same as in test set\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "games_target_test_ranking = label_encoder.fit_transform(games_target_test_ranking)\n",
    "\n",
    "# removed yellow and red cards\n",
    "# removed fouls and referee\n",
    "# removed shots and corners\n",
    "games_test_ranking = games_test_ranking.drop(columns=['HF','AF'])\n",
    "games_test_ranking = games_test_ranking.drop(columns=['HY','AY', 'HR', 'AR'])\n",
    "games_test_ranking = games_test_ranking.drop(columns=['Referee'])\n",
    "games_test_ranking = games_test_ranking.drop(columns=['HS','AS', 'HST', 'AST', 'HC', 'AC'])\n",
    "\n",
    "# remove full time goals\n",
    "games_test_ranking = games_test_ranking.drop(columns=['FTHG', 'FTAG'])\n",
    "\n",
    "# onehot HTR\n",
    "games_onehot_ranking = pd.DataFrame(encoder.fit_transform(games_test_ranking['HTR'].values.reshape(-1,1)).toarray(), columns=encoder.get_feature_names(['HTR']))\n",
    "games_test_ranking = games_test_ranking.drop(columns=['HTR'])\n",
    "games_test_ranking = games_test_ranking.join(games_onehot_ranking)\n",
    "\n",
    "# normalising + new attribute\n",
    "games_test_ranking[['HTHG', 'HTAG','HomePreviousRanking', 'AwayPreviousRanking']] = scaler.fit_transform(games_test_ranking[['HTHG', 'HTAG','HomePreviousRanking', 'AwayPreviousRanking']])\n",
    "\n",
    "# transforming home and awayteam with mapping from training set\n",
    "# higher values for more seasons, sheffield team for test set\n",
    "# transform teams not with one hot to avoid curse of dimensionality\n",
    "games_test_ranking['HomeTeam'] = games_test_ranking['HomeTeam'].map(d).fillna(games_test_ranking['HomeTeam'])\n",
    "\n",
    "games_test_ranking['AwayTeam'] = games_test_ranking['AwayTeam'].map(d).fillna(games_test_ranking['AwayTeam'])\n",
    "\n",
    "games_test_ranking.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with optimised parameters\n",
    "# DecisionTreeClassifier(criterion= 'gini', max_depth= 5, min_samples_split= 3), KNeighborsClassifier(n_neighbors = 7), GaussianNB(), SVC(gamma='auto'), MultinomialNB(alpha=0), RandomForestClassifier(bootstrap = True, max_depth = 100, max_features = 'sqrt', min_samples_leaf = 4, min_samples_split=5,n_estimators=100)]\n",
    "# accuracy without tuning: 49,31%, 55,56%, 57,99%, 56,25%, 57,63%, 58,68%\n",
    "\n",
    "pipeline = Pipeline([('estimator', None)])\n",
    "estimators = [ #optimized\n",
    "DecisionTreeClassifier(), KNeighborsClassifier(), GaussianNB(), SVC(gamma='auto'), MultinomialNB(), RandomForestClassifier()]\n",
    "\n",
    "# evaluate accuracy for training and test set\n",
    "for i, estimator in enumerate(estimators):\n",
    "    \n",
    "    pipeline.set_params(estimator=estimator)\n",
    "    accuracy= cross_val_score(pipeline, games_ranking, games_target_ranking, cv=cross_val, scoring='accuracy')\n",
    "    print(type(estimator).__name__)\n",
    "    print(\"Accuracy = {}%\".format(accuracy.mean() * 100.0))\n",
    "    \n",
    "    pipeline.fit(games_ranking, games_target_ranking)\n",
    "    test_prediction = pipeline.predict(games_test_ranking)\n",
    "    print(\"Test Accuracy: {}%\".format(accuracy_score(games_target_test_ranking, test_prediction)* 100.0))\n",
    "    print()\n",
    "\n",
    "classes = label_encoder.classes_\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "# Plot confusion matrix and classification report for optimized algorithms\n",
    "for i, estimator in enumerate(estimators):\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.subplot(3,2,i+1)\n",
    "    pipeline.set_params(estimator=estimator)\n",
    "    \n",
    "    prediction = cross_val_predict(pipeline, games_ranking, games_target_ranking, cv=cv)\n",
    "    cnf_matrix = confusion_matrix(games_target_ranking, prediction)\n",
    "    np.set_printoptions(precision=2)\n",
    "    plot_confusion_matrix(cnf_matrix, classes=classes, title = type(estimator).__name__)\n",
    "    rep = classification_report(games_target_ranking, prediction)\n",
    "\n",
    "    print('Classification report: {}'.format(type(estimator).__name__))\n",
    "    print(rep)   \n",
    "    \n",
    "    plt.show() \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set with New Attribute Shots and Corners converted to halftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new dataset with new ranking attribute\n",
    "games_shots = pd.read_csv('epl_season_2014-2019_shots.csv')\n",
    "\n",
    "# remove betting attributes and rename new columns\n",
    "games_shots.drop(games_shots.iloc[:, 31:76], inplace = True, axis = 1)\n",
    "games_shots = games_shots.drop(games_shots.index[380])\n",
    "\n",
    "# remove league and date\n",
    "games_shots.drop(columns='Div', axis=1, inplace=True)\n",
    "games_shots.drop(columns='Date', axis=1, inplace=True)\n",
    "\n",
    "# drop target\n",
    "games_target_shots = games_shots['FTR']\n",
    "games_shots = games_shots.drop(columns='FTR')\n",
    "\n",
    "# transform FTR #2=H,1=D,0=A but same as in test set\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "games_target_shots = label_encoder.fit_transform(games_target_shots)\n",
    "\n",
    "# removed yellow and red cards\n",
    "# removed fouls and referee\n",
    "# removed shots and corners\n",
    "games_shots = games_shots.drop(columns=['HF','AF', 'HTHF', 'HTAF'])\n",
    "games_shots = games_shots.drop(columns=['HY','AY', 'HR', 'AR'])\n",
    "games_shots = games_shots.drop(columns=['Referee'])\n",
    "games_shots = games_shots.drop(columns=['HS','AS', 'HST', 'AST', 'HC', 'AC'])\n",
    "\n",
    "# remove full time goals\n",
    "games_shots = games_shots.drop(columns=['FTHG', 'FTAG'])\n",
    "\n",
    "# onehot HTR\n",
    "games_onehot_shots = pd.DataFrame(encoder.fit_transform(games_shots['HTR'].values.reshape(-1,1)).toarray(), columns=encoder.get_feature_names(['HTR']))\n",
    "games_shots = games_shots.drop(columns=['HTR'])\n",
    "games_shots = games_shots.join(games_onehot_shots)\n",
    "\n",
    "# normalising + new attribute\n",
    "games_shots[['HTHG', 'HTAG','HTHST','HTAST', 'HTHS','HTAS', 'HTHC', 'HTAC' ]] = scaler.fit_transform(games_shots[['HTHG', 'HTAG','HTHST','HTAST', 'HTHS','HTAS', 'HTHC', 'HTAC']])\n",
    "\n",
    "# transforming home and awayteam with mapping from training set\n",
    "# higher values for more seasons, sheffield team for test set\n",
    "# transform teams not with one hot to avoid curse of dimensionality\n",
    "games_shots['HomeTeam'] = games_shots['HomeTeam'].map(d).fillna(games_shots['HomeTeam'])\n",
    "\n",
    "games_shots['AwayTeam'] = games_shots['AwayTeam'].map(d).fillna(games_shots['AwayTeam'])\n",
    "\n",
    "games_shots= games_shots.drop(games_shots.index[1899])\n",
    "\n",
    "# check for null values\n",
    "print(games_shots.isnull().values.sum())\n",
    "print(np.isnan(games_target_shots).any())\n",
    "games_target_shots = np.delete(games_target_shots, 1899)\n",
    "\n",
    "games_shots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new test with new ranking attribute\n",
    "games_test_shots = pd.read_csv('test_season_2019-2020_shots.csv')\n",
    "\n",
    "# remove betting attributes\n",
    "games_test_shots.drop(games_test_shots.iloc[:, 32:114], inplace = True, axis = 1)\n",
    "\n",
    "# remove league and date and time\n",
    "games_test_shots.drop(columns='Div', axis=1, inplace=True)\n",
    "games_test_shots.drop(columns='Date', axis=1, inplace=True)\n",
    "games_test_shots.drop(columns='Time', axis=1, inplace=True)\n",
    "\n",
    "# drop target\n",
    "games_target_test_shots = games_test_shots['FTR']\n",
    "games_test_shots = games_test_shots.drop(columns='FTR')\n",
    "\n",
    "# transform FTR #2=H,1=D,0=A but same as in test set\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "games_target_test_shots = label_encoder.fit_transform(games_target_test_shots)\n",
    "\n",
    "# removed yellow and red cards\n",
    "# removed fouls and referee\n",
    "# removed shots and corners\n",
    "games_test_shots = games_test_shots.drop(columns=['HF','AF','HTHF', 'HTAF'])\n",
    "games_test_shots = games_test_shots.drop(columns=['HY','AY', 'HR', 'AR'])\n",
    "games_test_shots = games_test_shots.drop(columns=['Referee'])\n",
    "games_test_shots = games_test_shots.drop(columns=['HS','AS', 'HST', 'AST', 'HC', 'AC'])\n",
    "\n",
    "# remove full time goals\n",
    "games_test_shots = games_test_shots.drop(columns=['FTHG', 'FTAG'])\n",
    "\n",
    "# onehot HTR\n",
    "games_onehot_shots = pd.DataFrame(encoder.fit_transform(games_test_shots['HTR'].values.reshape(-1,1)).toarray(), columns=encoder.get_feature_names(['HTR']))\n",
    "games_test_shots = games_test_shots.drop(columns=['HTR'])\n",
    "games_test_shots = games_test_shots.join(games_onehot_ranking)\n",
    "\n",
    "# normalising + new attribute\n",
    "\n",
    "games_test_shots[['HTHG', 'HTAG','HTHST','HTAST', 'HTHS','HTAS','HTHC', 'HTAC']] = scaler.fit_transform(games_test_shots[['HTHG', 'HTAG','HTHST','HTAST', 'HTHS','HTAS','HTHC', 'HTAC']])\n",
    "\n",
    "# transforming home and awayteam with mapping from training set\n",
    "# higher values for more seasons, sheffield team for test set\n",
    "# transform teams not with one hot to avoid curse of dimensionality\n",
    "games_test_shots['HomeTeam'] = games_test_shots['HomeTeam'].map(d).fillna(games_test_shots['HomeTeam'])\n",
    "\n",
    "games_test_shots['AwayTeam'] = games_test_shots['AwayTeam'].map(d).fillna(games_test_shots['AwayTeam'])\n",
    "\n",
    "games_test_shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with optimised parameters:\n",
    "# DecisionTreeClassifier(criterion= 'gini', max_depth= 5, min_samples_split= 3), KNeighborsClassifier(n_neighbors = 7), GaussianNB(), SVC(C = 1, gamma=0.1, kernel ='linear'), MultinomialNB(alpha=0), RandomForestClassifier(bootstrap = True, max_depth = 100, max_features = 'sqrt', min_samples_leaf = 4, min_samples_split=5,n_estimators=100)\n",
    "# accuracy without tuning: 45,83%, 44,79%, 44,44%, 51,04%, 48,26%, 60,10%\n",
    "\n",
    "pipeline = Pipeline([('estimator', None)])\n",
    "estimators = [ #optimized\n",
    "    DecisionTreeClassifier(), KNeighborsClassifier(), GaussianNB(), SVC(gamma='auto'), MultinomialNB(), RandomForestClassifier()\n",
    "]\n",
    "\n",
    "# evaluate accuracy for training and test set\n",
    "for i, estimator in enumerate(estimators):\n",
    "    \n",
    "    pipeline.set_params(estimator = estimator)\n",
    "    accuracy= cross_val_score(pipeline, games_shots, games_target_shots, cv=cross_val, scoring='accuracy')\n",
    "    \n",
    "    print(type(estimator).__name__)\n",
    "    print(\"Accuracy = {}%\".format(accuracy.mean() * 100.0))\n",
    "    \n",
    "    pipeline.fit(games_shots, games_target_shots)\n",
    "    test_prediction = pipeline.predict(games_test_shots)\n",
    "    print(\"Test Accuracy: {}%\".format(accuracy_score(games_target_test_shots, test_prediction)* 100.0))\n",
    "    print()\n",
    "\n",
    "classes = label_encoder.classes_\n",
    "\n",
    "# Plot confusion matrix and classification report for optimized algorithms\n",
    "for i, estimator in enumerate(estimators):\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.subplot(3,2,i+1)\n",
    "    pipeline.set_params(estimator=estimator)\n",
    "    \n",
    "    prediction = cross_val_predict(pipeline, games_shots, games_target_shots, cv=cv)\n",
    "    cnf_matrix = confusion_matrix(games_target_shots, prediction)\n",
    "    np.set_printoptions(precision=2)\n",
    "    plot_confusion_matrix(cnf_matrix, classes=classes, title = type(estimator).__name__)\n",
    "    rep = classification_report(games_target_shots, prediction)\n",
    "\n",
    "    print('Classification report: {}'.format(type(estimator).__name__))\n",
    "    print(rep)   \n",
    "    \n",
    "    plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Result\n",
    "\n",
    "- baseline: random guessing: 33.33% or major class: 44.79%\n",
    "\n",
    "- best model: RandomForest optimized: 62.15% on basic training data with just halftime attributes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
